{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bsaze9aGj16",
        "outputId": "cbab29e5-fa57-4618-8a02-ebc56c8051b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w-dCnJCgs78"
      },
      "outputs": [],
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA0U5elQJOJG",
        "outputId": "e184ec17-4185-4fdb-9d44-bc084a81dc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uNOS2VWKnFu",
        "outputId": "9508e9dd-edad-45dc-cd63-40f7beceb20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ],
      "source": [
        "nltk.download('book')\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Ed10YSOZFC",
        "outputId": "d83f1300-4aa3-4387-c504-0173f3e15ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text from Moby Dick:\n",
            "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', 'He', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '\"', 'While', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',']\n"
          ]
        }
      ],
      "source": [
        "#first few words of Moby Dick\n",
        "print(\"Sample text from Moby Dick:\")\n",
        "print(text1[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8D3jIslNmUa",
        "outputId": "bde34d9f-9af8-42f7-a88f-877f23faccb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 19317\n",
            "Number of unique words (including punctuation): 19317\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# 1(a) Vocabulary Size\n",
        "vocabulary_size = len(set(text1))\n",
        "print(\"Vocabulary Size:\", vocabulary_size)\n",
        "\n",
        "# or unique words\n",
        "word_freq = FreqDist(text1)\n",
        "print(\"Number of unique words (including punctuation):\", len(word_freq))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074kf6gdcz0J",
        "outputId": "31e356b8-7dff-4545-f262-ea8ac8461fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 260819\n"
          ]
        }
      ],
      "source": [
        "# (b) Total Words\n",
        "total_words = len(text1)\n",
        "print(\"Total Words:\", total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24bFpykIenr1",
        "outputId": "580ab796-20c2-4962-b3d2-f73ae5ab5a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequencies saved to 'word_frequencies.txt'\n"
          ]
        }
      ],
      "source": [
        "# (c) the number of times a word is present in the novella\n",
        "word_freq = FreqDist(text1)\n",
        "word_count_dict = dict(word_freq)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/440/word_frequencies.txt\", \"w\") as f:\n",
        "    for word, count in word_count_dict.items():\n",
        "        f.write(f\"{word}: {count}\\n\")\n",
        "\n",
        "print(\"Word frequencies saved to 'word_frequencies.txt'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkTxI4Q8fpfA"
      },
      "source": [
        "link to the **word_frequencies.txt** file: https://drive.google.com/file/d/1ualNSoEfzcLWiLUylNzPIwC1NWDC8d_K/view?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsEDV8VVc2VL",
        "outputId": "d80fdc0e-c943-4e41-99c5-830dbce9a9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words (including punctuation):\n",
            "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n"
          ]
        }
      ],
      "source": [
        "# (d) Top 10 Most Frequent Words (Including Punctuation)\n",
        "top_10_words = word_freq.most_common(10)\n",
        "print(\"Top 10 Most Frequent Words (including punctuation):\")\n",
        "print(top_10_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U_zuS1id2ok",
        "outputId": "f32f8c93-d772-4b13-ec84-16ca73bf9110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words (excluding punctuation):\n",
            "[('the', 13721), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), ('in', 3916), ('that', 2982), ('his', 2459), ('it', 2209), ('I', 2124)]\n"
          ]
        }
      ],
      "source": [
        "# (e) Top 10 Most Frequent Words (Excluding Punctuation)\n",
        "words_only = [word for word in text1 if word.isalpha()]\n",
        "word_freq_no_punct = FreqDist(words_only)\n",
        "\n",
        "top_10_no_punct = word_freq_no_punct.most_common(10)\n",
        "print(\"Top 10 Most Frequent Words (excluding punctuation):\")\n",
        "print(top_10_no_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKs-rzjdrDTe"
      },
      "outputs": [],
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Aw5292uIjbR",
        "outputId": "898dac0b-6d14-4154-8013-827ac423ea0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of reviews: 50000\n",
            "\n",
            "Longest review in terms of words:\n",
            "Row: 40521\n",
            "review            There's a sign on The Lost Highway that says:<...\n",
            "sentiment                                                  positive\n",
            "word_count                                                     2911\n",
            "sentence_count                                                   65\n",
            "Name: 40521, dtype: object\n",
            "\n",
            "Longest review in terms of sentences:\n",
            "Row: 39182\n",
            "review            Smallville episode Justice is the best episode...\n",
            "sentiment                                                  positive\n",
            "word_count                                                      298\n",
            "sentence_count                                                  282\n",
            "Name: 39182, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# (a)\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/440/IMDB Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# total number of reviews\n",
        "total_reviews = len(df)\n",
        "print(f\"Total number of reviews: {total_reviews}\")\n",
        "\n",
        "df['word_count'] = df['review'].apply(lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0)\n",
        "df['sentence_count'] = df['review'].apply(lambda x: len(sent_tokenize(x)) if isinstance(x, str) else 0)\n",
        "\n",
        "longest_review_words_index = df['word_count'].idxmax()\n",
        "longest_review_sentences_index = df['sentence_count'].idxmax()\n",
        "\n",
        "longest_review_words = df.loc[longest_review_words_index]\n",
        "longest_review_sentences = df.loc[longest_review_sentences_index]\n",
        "\n",
        "print(\"\\nLongest review in terms of words:\")\n",
        "print(f\"Row: {longest_review_words_index}\")\n",
        "print(longest_review_words)\n",
        "\n",
        "print(\"\\nLongest review in terms of sentences:\")\n",
        "print(f\"Row: {longest_review_sentences_index}\")\n",
        "print(longest_review_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0IRw4_ENjHj",
        "outputId": "8557e061-caa7-4300-8eb6-0f390fba2455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        " # b\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_review(text):\n",
        "    if isinstance(text, str):\n",
        "        text_no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        word_tokens = text_no_punctuation.split()\n",
        "        filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return \" \".join(filtered_tokens)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(clean_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_Jaab1WWljl",
        "outputId": "b1976b4d-8994-4ec4-c0b2-40b4010d3b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 50000 reviews successfully saved to /content/drive/MyDrive/440/cleaned_reviews.txt.\n"
          ]
        }
      ],
      "source": [
        "# c\n",
        "\n",
        "if 'cleaned_review' in df.columns:\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: x.lower() if isinstance(x, str) else \"\")\n",
        "\n",
        "    output_text_file = '/content/drive/MyDrive/440/cleaned_reviews.txt'\n",
        "    with open(output_text_file, 'w', encoding='utf-8') as f:\n",
        "        for review in df['cleaned_review']:\n",
        "            f.write(review.strip() + '\\n')  # Writing each review as a new line in the text file\n",
        "\n",
        "\n",
        "#     --------------\n",
        "    with open(output_text_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if len(lines) == len(df):\n",
        "        print(f\"All {len(lines)} reviews successfully saved to {output_text_file}.\")\n",
        "    else:\n",
        "        print(f\"Some reviews might be missing. Saved {len(lines)} out of {len(df)} reviews.\")\n",
        "else:\n",
        "    print(\"Column 'cleaned_review' not found in the dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L__jiZS-vbb1"
      },
      "source": [
        "link to the **cleaned_reviews.txt** file: https://drive.google.com/file/d/135yXHFZx6YqsEZRZA1IAucJzfuYu-umB/view?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "CTMy-lnD3YM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n"
      ],
      "metadata": {
        "id": "xRoihvn7bRbI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names_file = \"/content/drive/MyDrive/440/tfidf_feature_names.txt\"\n",
        "matrix_file = \"/content/drive/MyDrive/440/tfidf_matrix_sparse.txt\"\n",
        "\n",
        "with open(feature_names_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(\"Feature Names:\\n\")\n",
        "    file.write(\"\\n\".join(vectorizer.get_feature_names_out()))\n",
        "\n",
        "\n",
        "# sparse matrix\n",
        "with open(matrix_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(\"TF-IDF Sparse Matrix:\\n\")\n",
        "    for i, j in zip(*tfidf_matrix.nonzero()):  # nonzero() for sparse indices\n",
        "        file.write(f\"({i}, {j}) {tfidf_matrix[i, j]}\\n\")\n",
        "\n",
        "\n",
        "print(\"TF-IDF vectorization complete using sparse operations.\")\n",
        "print(f\"Feature names saved to {feature_names_file}.\")\n",
        "print(f\"Sparse TF-IDF Matrix saved to {matrix_file}.\")\n"
      ],
      "metadata": {
        "id": "z4dRWG23f7Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad8ba91-eedf-4163-e2c7-b4d1213d939f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorization complete using sparse operations.\n",
            "Feature names saved to /content/drive/MyDrive/440/tfidf_feature_names.txt.\n",
            "Sparse TF-IDF Matrix saved to /content/drive/MyDrive/440/tfidf_matrix_sparse.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link to [**tfidf_feature_names.txt**](https://drive.google.com/file/d/1josOj4qcNWkfjRBZSYChD09GwAFjyvzS/view?usp=drive_link) file   and   \n",
        "link to [**tfidf_feature_names.txt**](https://drive.google.com/file/d/1yC0VHtj5gxeI9ihw3qZj0p4qtCH3IYnO/view?usp=drive_link) file"
      ],
      "metadata": {
        "id": "mliBOx1GsvGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_shape = tfidf_matrix.shape\n",
        "\n",
        "print(f\"The shape of the TF-IDF matrix is: {matrix_shape}\")\n",
        "print(f\"Number of documents: {matrix_shape[0]}\")\n",
        "print(f\"Number of features (vocabulary size): {matrix_shape[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY8yqgyCpC23",
        "outputId": "f21034f4-7fd2-4d3b-c9f6-1884876b39da"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the TF-IDF matrix is: (50000, 180355)\n",
            "Number of documents: 50000\n",
            "Number of features (vocabulary size): 180355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0B8v3mBfzQ2",
        "outputId": "0311f9fa-a334-4ae4-e54a-467dfee333fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words loaded: 400000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# b\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "glove_file = \"/content/drive/MyDrive/440/glove.6B.100d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_file)\n",
        "\n",
        "print(f\"Total words loaded: {len(glove_embeddings)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKgXq_b7kxKS",
        "outputId": "75b1a7b3-b2ca-41ce-a5a3-6842983a5fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All word vectors have been saved to /content/drive/MyDrive/440/glove_vectors.txt.\n"
          ]
        }
      ],
      "source": [
        "output_file = \"/content/drive/MyDrive/440/glove_vectors.txt\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for word, vector in glove_embeddings.items():\n",
        "        vector_string = \" \".join(map(str, vector))\n",
        "        f.write(f\"{word} {vector_string}\\n\")\n",
        "\n",
        "print(f\"All word vectors have been saved to {output_file}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGS6UHfF0NS4"
      },
      "source": [
        "link to the **glove_vectors.txt** file: https://drive.google.com/file/d/1wWokLW48dzOlXuoRGKX2NWOV-banLQpr/view?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmEwIfkHzYGr",
        "outputId": "1dce5cd5-9ce3-4cf2-87c3-7dbb89ea344f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarities:\n",
            "man_woman: 0.8323\n",
            "cat_dog: 0.8798\n",
            "king_queen: 0.7508\n",
            "\n",
            "Similarity between King - Man + Woman and Queen: 0.7834\n"
          ]
        }
      ],
      "source": [
        "words = [\"man\", \"woman\", \"cat\", \"dog\", \"king\", \"queen\"]\n",
        "vectors = {word: glove_embeddings[word] for word in words}\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarities = {\n",
        "    \"man_woman\": cosine_similarity(vectors[\"man\"], vectors[\"woman\"]),\n",
        "    \"cat_dog\": cosine_similarity(vectors[\"cat\"], vectors[\"dog\"]),\n",
        "    \"king_queen\": cosine_similarity(vectors[\"king\"], vectors[\"queen\"]),\n",
        "}\n",
        "\n",
        "king_minus_man_plus_woman = vectors[\"king\"] - vectors[\"man\"] + vectors[\"woman\"]\n",
        "\n",
        "similarity_to_queen = cosine_similarity(king_minus_man_plus_woman, vectors[\"queen\"])\n",
        "\n",
        "print(\"Cosine Similarities:\")\n",
        "for pair, similarity in similarities.items():\n",
        "    print(f\"{pair}: {similarity:.4f}\")\n",
        "\n",
        "# print(\"Queen:\", vectors[\"queen\"])\n",
        "# print(\"King - Man + Woman:\", king_minus_man_plus_woman)\n",
        "print(f\"\\nSimilarity between King - Man + Woman and Queen: {similarity_to_queen:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlBJgMt80aAZ"
      },
      "source": [
        "Yes, they reflect what we learnt in class as a cosine similarity close to 1 (0.7834) indicates that the result vector is very similar to the vector for \"Queen.\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}